# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_eueMS1Suf9TgWXBV2ge0iBC9h2ZDlQF

## **              SPAM MAİL CLASSİFİER**

Yeni teknik ilerleme çağında, elektronik postalar (e-postalar) profesyonel, ticari ve kişisel iletişim için önemli miktarda kullanıcı topladı. 2019'da ortalama olarak her birey her gün 130 e-posta alıyordu ve o yıl toplamda 296 Milyar e-posta gönderildi.Yüksek talep ve büyük kullanıcı tabanı nedeniyle, istenmeyen e-postalarda, aynı zamanda spam e-postalar olarak da bilinen bir artış var. Spam e-postaların toplam e-postalara oranının %50'ye kadar çıktığı görülmüştür.
"""

import pandas as pd
import numpy as np
import string
import nltk
from nltk.corpus import stopwords
import nltk
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
from nltk.tokenize import word_tokenize
from collections import Counter
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn import svm
import seaborn as sns

Veri setleri kaggle.com dan alınmıştır.
Sample sayısını ve çeşitliliği arttırabilmek için iki veri seti birleştirilmiştir.
Dataset1:https://www.kaggle.com/itsbhups/98-accuracy-spam-ham-messages/data
Dataset2:https://www.kaggle.com/venky73/spam-mails-dataset
veri setinin boyutları:10688,2

data=pd.read_csv("Email spam.csv")
data.rename(columns={"spam":"target"},inplace=True)
data1=pd.read_csv("spam_ham_dataset.csv")
data1.drop(["label","Unnamed: 0"],axis=1,inplace=True)
data1.rename(columns={"label_num":"target"},inplace=True)
datadf=pd.concat([data,data1],axis=0)
datadf.reset_index(inplace=True,drop=True)
datadf.index=range(datadf.shape[0])
datadf=datadf.drop_duplicates()
datadf.head(10)

#Verinin incelenmesi
sns.countplot(x ='target', data = datadf,palette="PuBuGn_r")
plt.xlabel("Label")
plt.ylabel("Count")
plt.show()

#Text sütunun ilk kelimesi olan Subject silinir.
for i in range(len(datadf)):
    datadf.iloc[i, 0] = datadf.iloc[i, 0].split(' ', 1)[1]

#tüm kelimelerin küçük harfe çevrilmesi
datadf['text']= datadf['text'].apply(lambda x: x.lower())

#Stopword temizlenmesi
stop_words = set(stopwords.words('english') + ['u', 'ü', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure'])
datadf['text']=datadf['text'].apply(lambda x: " ".join(term for term in x.split() if term not in stop_words))


#Noktalama işaretlerinin temizlenmesi
def remove_punctuation(text):
    punctuationfree="".join([i for i in text if i not in string.punctuation])
    return punctuationfree
datadf['text']= datadf['text'].apply(lambda x:remove_punctuation(x))


#tek harflerin silinmesi
def remove_singlechar(text):
    singlecharfree="".join([i for i in text if len(i)<=1])
    return singlecharfree
datadf['text']= datadf['text'].apply(lambda x:remove_singlechar(x))

"""Stemming, bir kelimenin son birkaç karakterini kaldırır veya köklendirir, bu da genellikle yanlış anlamlara ve yazımlara yol açar. Lemmatizasyon, bağlamı dikkate alır ve kelimeyi Lemma adı verilen anlamlı temel formuna dönüştürür.
Lemmatizasyon için WordNet ten POS etiketleri kullanılır. En doğru sonuca bu şekilde ulaşır.
"""

print(datadf.head())
datadf["text"]=datadf['text'].apply(word_tokenize)
def get_pos( word ):
    w_synsets = wordnet.synsets(word)
    pos_counts = Counter()
    pos_counts["n"] = len(  [ item for item in w_synsets if item.pos()=="n"]  )
    pos_counts["v"] = len(  [ item for item in w_synsets if item.pos()=="v"]  )
    pos_counts["a"] = len(  [ item for item in w_synsets if item.pos()=="a"]  )
    pos_counts["r"] = len(  [ item for item in w_synsets if item.pos()=="r"]  )
    most_common_pos_list = pos_counts.most_common(3)
    return most_common_pos_list[0][0]
wordnet_lemmatizer = WordNetLemmatizer()
def lemmatizer(text):
    lemm_text = [wordnet_lemmatizer.lemmatize(word,get_pos(word)) for word in text]
    return lemm_text
datadf['text']=datadf['text'].apply(lambda x:lemmatizer(x))
def concanatee(text):
    listToStr = ' '.join([str(elem) for elem in text])
    return listToStr
datadf['text']=datadf['text'].apply(lambda x:concanatee(x))

"""Bag of Words, belgede geçen kelime sayısını içeren bir dizi vektör oluştururken, TF-IDF modeli daha önemli ve daha az önemli kelimeler hakkında bilgi içerir.Bag of Words vektörlerinin yorumlanması kolaydır. Ancak, TF-IDF genellikle makine öğrenimi modellerinde daha iyi performans gösterir."""

vectorizer = TfidfVectorizer()
vectors = vectorizer.fit_transform(datadf['text'])
features = vectors

X_train, X_test, y_train, y_test = train_test_split(features, datadf['target'], test_size=0.2)
sonuc=[]
algo=["KNN","Random Forest","SVM"]

#KNN
k_range = range(3,22,2)
scores = []
for k in k_range:
    knn = KNeighborsClassifier(n_neighbors = k)
    knn.fit(X_train, y_train)
    scores.append(knn.score(X_test, y_test))

plt.figure()
plt.xlabel('k')
plt.ylabel('accuracy')
plt.scatter(k_range, scores)
plt.xticks(range(3,22,2))
plt.show()

knn = KNeighborsClassifier(n_neighbors=11)#3-21 arası değerler denendi max accuracy 11 de geldiği için tercih edildi
knn.fit(X_train, y_train)
y_pred2 = knn.predict(X_test)
print("KNN Accuracy:",metrics.accuracy_score(y_test, y_pred2))
sonuc.append(metrics.accuracy_score(y_test, y_pred2))
report2 = metrics.classification_report(y_test, y_pred2)
print("KNN other metrics :")
print(report2)
matrix2 = metrics.confusion_matrix(y_test, y_pred2)
print("KNN Confusion Matrix :")
print(matrix2)

n_range = range(1, 30)
scores = []
for n in n_range:
    clf=RandomForestClassifier(n_estimators=n)
    clf.fit(X_train,y_train)
    scores.append(clf.score(X_test, y_test))
plt.figure()
plt.xlabel('Number of Trees')
plt.ylabel('Accuracy')
plt.scatter(n_range, scores)
plt.xticks(range(1, 30))


clf=RandomForestClassifier(n_estimators=28) # 3-30 arası değerleri denedii en yüksek doğruluğu 18 verdi
clf.fit(X_train,y_train)
y_pred=clf.predict(X_test)
print("Random Forest Accuracy:",metrics.accuracy_score(y_test, y_pred))
report = metrics.classification_report(y_test, y_pred)
sonuc.append(metrics.accuracy_score(y_test, y_pred))
print("Random Forest other metrics :")
print(report)
matrix = metrics.confusion_matrix(y_test, y_pred)
print("Random Forest Confusion Matrix :")
print(matrix)

kernel_type=["linear","rbf","poly"]
scores = []
for k in kernel_type:
    svmclf = svm.SVC(kernel=k,probability=True) #  rbf linear ve poly kernel tipleri denenip linear seçilmiştir.
    svmclf.fit(X_train, y_train)
    y_pred3 = svmclf.predict(X_test)
    scores.append(metrics.accuracy_score(y_test, y_pred3))
plt.bar(kernel_type, scores)
plt.xlabel('k')
plt.ylabel('accuracy')

plt.show()


svmclf = svm.SVC(kernel='linear',probability=True) #  rbf linear ve poly kernel tipleri denenip linear seçilmiştir.
svmclf.fit(X_train, y_train)
y_pred3 = svmclf.predict(X_test)

print("SVM Accuracy:",metrics.accuracy_score(y_test, y_pred3))
report3 = metrics.classification_report(y_test, y_pred3)
sonuc.append(metrics.accuracy_score(y_test, y_pred3))
print("SVM other metrics :")
print(report3)
matrix3 = metrics.confusion_matrix(y_test, y_pred3)
print("SVM Confusion Matrix :")
print(matrix3)

plt.bar(algo,sonuc,color="salmon")
plt.xlabel('Algorithms')
plt.ylabel('Accuracy')
plt.show()